---
title: 'Practical Machine Learning: Course Project'
author: "Jeff Iacono"
date: "July 27, 2014"
output: html_document
---

**Predicting _classe_ from [groupware](http://groupware.les.inf.puc-rio.br/har)
data set.**

Note that you can view the (github repo that generates this file and houses other scripts here)[https://github.com/jeffreyiacono/pml/tree/master].

To start, I loaded up the caret package and the training data, normalizing some
of the error values (ex: "" (blank), "#DIV/0!", etc.) to be NAs:

```
# install + load caret
install.packages("caret")
library(caret)

# read training data in, normalize bad values
plmTrain <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA","", "#DIV/0!"))
```

I then removed any columns that were missing data:

```
# remove any cols that are missing data
plmTrain <- plmTrain[, names(plmTrain)[sapply(plmTrain, function(x) { !any(is.na(x)) })]]
```

Next, I dropped the following columns:

```
  index, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp,
  new_window, num_window
```

These columns were either categorical or did not seemingly provide any
predictive ability. This was accomplished via:

```
# remove non-numerics
plmTrain <- plmTrain[, -c(1, 2, 3, 4, 5, 6, 7)]
```

With the training data set now cleaned, I moved on to splitting into training
(70%) and sub-testing (30%) sets:

```
# create training / test
inTrain <- createDataPartition(y = plmTrain$classe, p = 0.7, list = FALSE)
training <- plmTrain[inTrain, ]
testing <- plmTrain[-inTrain, ]
```

Next, I wanted to further inspect our 53 remaining features. I checked
to see if any had little to no variation and thus would be good candidates for
exclusion:

```
nearZeroVar(training[, -53], saveMetrics = TRUE)
```

None reported a zeroVar or nzv of TRUE, so none are good candidates for
exclusion based on variation levels.

To inspect if any of our features were highly correlated, I created a
correlation matrix and selected those that had 95% correlation or greater:

```
M <- abs(cor(training[, -53]))
diag(M) <- 0
which(M > 0.95, arr.ind = TRUE)
```

This returned a total of 8 unique features, so I decided to proceed with
principle component analysis (PCA) in order to refine our features down:

```
# principle components + model
preProc <- preProcess(training[, -53], method = "pca", threshold = 0.95)
trainPC <- predict(preProc, training[,-53])
```

PCA needed 25 components to capture 95 percent of the variance, so we've further
refined our feature set down.

Now it was time to train our model. I opted for a Random Forest model using the
Caret package, making sure to set the data attribute to the trainPC variable
that contains the new principle components calculated in the prior step:

```
modelFit <- train(training$classe ~ ., method = "rf", data = trainPC)
```

This process bootstrapped 25 times and produced a final model with an **out of
sample error estimate of 2.35%**.

To cross-validate, I calculated the sub-tests's principle components and
predicted against this data set, using the model from the prior step:

```
# test
testPC <- predict(preProc, testing[, -53])
```

I then calculated the confusionMatrix to observe the results:

```
confusionMatrix(testing$classe, predict(modelFit, testPC))
```

Overall, **the accuracy was 0.9777**, which is inline with our expectations for
out of sample error.

Finally, it was time to predict our 20 outcomes off of the test input file. I
read in the file, trimmed down the features to only those that we have been
using, predicted the principle components, and then predicted the classe
outcome:

```
plmTest <- read.csv("pml-testing.csv", header = TRUE)
plmTestPC <- predict(preProc, plmTest[, names(training[, -53])])
predict(modelFit, plmTestPC)
```

The model ended up predicting 20 / 20!